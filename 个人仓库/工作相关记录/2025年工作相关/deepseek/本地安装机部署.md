### 1、Ollama与deepseek在本地部署的时候起的作用机关系
1. ollama是LLM(大语言模型)的运行平台
	- **下载和管理模型**：Ollama 可以直接下载和管理 DeepSeek 模型，不需要手动配置依赖。
	- **优化推理**：它对 LLM 进行了**优化和量化**，能够让大模型在本地设备上更高效地运行（例如，支持 **GGUF 量化格式**）。
	- **提供 API**：Ollama 允许通过简单的 **API**（本地 HTTP 服务器）与 DeepSeek 模型交互，使其易于集成到其他应用中。
2. deepseek是大语言模型
	- DeepSeek 是 Ollama 运行的 **LLM 模型**，可以提供文本生成、问答、代码生成等能力。
	- 你可以选择不同大小的 DeepSeek 模型（例如 **DeepSeek 7B、DeepSeek 67B**）。
	- DeepSeek 本身只是一个**模型文件**，它需要一个推理框架（如 **Ollama、vLLM、Text Generation WebUI**）来运行。
### 2、本地部署大模型
```shell
//首先先下载大模型，后续运行也使用此命令
ollama run deepseek-r1:8b
//如果使用dify的话需要使用到文本嵌入模型，使用如下命令进行拉取，这个模型主要用于本地知识库导入的时候使用高质量模式进行分割
ollama pull nomic-embed-text
```
### 3、在docker中部署dify
```shell
//首先先拉取dify镜像
git clone https://github.com/langgenius/dify.git --branch 0.15.3
//进入目录将.env.example复制一份重命名为.env

//然后在dify/docker目录中cmd使用如下命令拉去dify所需的镜像并启动
docker compose up -d

//通过docker ps 命令可以查看当前容器的一些信息

//使用浏览器进入127.0.0.1:80就可以进入dify的前端
```